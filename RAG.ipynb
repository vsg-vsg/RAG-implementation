{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d05edd0d",
   "metadata": {},
   "source": [
    "<h1 style=\"color: brown; font-size: xx-large\">RAG Implementation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a838e",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "### Let us start my creating a sample corpus for our use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15108f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corpus = [\n",
    "    \"Read a book at a cozy caf√© and get lost in a new story.\",\n",
    "    \"Take a pottery class and create something unique with your hands.\",\n",
    "    \"Join a cooking class and learn how to make gourmet dishes.\",\n",
    "    \"Go for a bike ride through the countryside and enjoy the open air.\",\n",
    "    \"Attend a painting workshop and unleash your creative side.\",\n",
    "    \"Take a weekend road trip to explore nearby towns and landmarks.\",\n",
    "    \"Visit the palace in the city.\",\n",
    "    \"Go to a comedy club and enjoy an evening of laughter.\",\n",
    "    \"Take a photography tour and capture beautiful landscapes.\",\n",
    "    \"Volunteer at a local shelter and make a difference in the community.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa9207",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "### Now we need a similarity measure to check the input imput with our corpus so that it returns the most relevant response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94414a3b",
   "metadata": {},
   "source": [
    "### Jaccard similarity uses set theory to get the similarity between the texts, so the texts have to be precprocessed first to remove the unnecessary formatting and then convert them to sets to perform the check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ab55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def get_jaccard_similarity(query, doc):\n",
    "    # Preprocessing\n",
    "    query_terms = set(preprocess_text(query))\n",
    "    doc_terms = set(preprocess_text(doc))\n",
    "    \n",
    "    # Intersection and Union of the sets for similarity checks\n",
    "    intersect_terms = query_terms.intersection(doc_terms)\n",
    "    union_terms = query_terms.union(doc_terms)\n",
    "    \n",
    "    # To handle cases where both sets are empty so that denominator is not empty\n",
    "    if len(union_terms) == 0:\n",
    "        return 0  \n",
    "    \n",
    "    return len(intersect_terms) / len(union_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cdbc3",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "### Now that we have our usable functions, we can retrieve the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4bd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query, corpus):\n",
    "    similarity_score_list = []\n",
    "    for doc in corpus:\n",
    "        score = get_jaccard_similarity(query, doc)\n",
    "        similarity_score_list.append(score)\n",
    "        \n",
    "    best_match_index = similarity_score_list.index(max(similarity_score_list))\n",
    "    return corpus[best_match_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3626e",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "### Our simple RAG is ready and we can test it on a sample user input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66e700",
   "metadata": {},
   "source": [
    "Let's say that the chatbot asks the user the question \\\n",
    "    **\"What is an activity of your interest?\"**\\\n",
    "\\\n",
    "The user's respose is **\"I like to visit places\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc77e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I like comedy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8359827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go to a comedy club and enjoy an evening of laughter.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(user_input, sample_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a45499",
   "metadata": {},
   "source": [
    "As the user likes to visit places, the suggestion was to visit the palace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02236b",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "<h3 style=\"color: blue; font-size: x-large\"> The retrieval impementation is taken care of and now it can be paired with an LLM<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8f202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_document = get_response(user_input, sample_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46ec92",
   "metadata": {},
   "source": [
    "### The retrieval implemented till now handles fetching the relevant document to the user input and we have to feed this to the LLM for it to generate it's response to the user's query\n",
    "\n",
    "### The feed prompt to the LLM has to be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6d355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_prompt = f\"\"\"\n",
    "You are an assistant providing activity suggestions. Answer in one short sentence only.\n",
    "Here is the suggested activity: {relevant_document}\n",
    "The user mentioned: {user_input}\n",
    "Compile a recommendation to the user in general based on the recommended activity and the user input in one short sentence only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b10cb",
   "metadata": {},
   "source": [
    "### We can use Hugging Face to load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2654c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9e2537",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c2178",
   "metadata": {},
   "source": [
    "### Let us generate the reponse using the prompt provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5528fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\nYou are an assistant providing activity suggestions. Answer in one short sentence only.\\nHere is the suggested activity: Go to a comedy club and enjoy an evening of laughter.\\nThe user mentioned: I like comedy\\nCompile a recommendation to the user in general based on the recommended activity and the user input in one short sentence only.\\nYou are an assistant providing activity suggestions. Answer in one short sentence only.\\nNow, at this point, you would like to answer more questions. How do you create such a recommendation? Well, the problem is... If I don't get anything out of it, I can't give it to the user. Also, it is not feasible for me to share it as a resource.\\nNow as expected, the task becomes a challenge. There are several ways about what to do. This is mainly when you are working on your own work and want to help others. You can use this information to help others. You can also tell yourself that you\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generator(llm_prompt, max_length=200, num_return_sequences=1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6110e2",
   "metadata": {},
   "source": [
    "### User only needs the generated text, so let us provide only the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "144ad2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are an assistant providing activity suggestions. Answer in one short sentence only.\\nHere is the suggested activity: Go to a comedy club and enjoy an evening of laughter.\\nThe user mentioned: I like comedy\\nCompile a recommendation to the user in general based on the recommended activity and the user input in one short sentence only.\\nYou are an assistant providing activity suggestions. Answer in one short sentence only.\\nNow, at this point, you would like to answer more questions. How do you create such a recommendation? Well, the problem is... If I don't get anything out of it, I can't give it to the user. Also, it is not feasible for me to share it as a resource.\\nNow as expected, the task becomes a challenge. There are several ways about what to do. This is mainly when you are working on your own work and want to help others. You can use this information to help others. You can also tell yourself that you\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e5571",
   "metadata": {},
   "source": [
    "### The reponse is out of context due to the usage of an older outdated model, a better model will yield a better result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
